# -*- coding: utf-8 -*-
"""Cohort Analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WJNKu8NfKBlgBiMPZB-Ggt4YddUb5y-_

# Bikes & Cycling Accessories Organisation's Transactions Data Based Cohort Analysis

## Dataset loading and cleaning
"""

# Importing Libraries
import pandas as pd
import seaborn as sns

import matplotlib.pyplot as plt
import numpy as np
import datetime as dt

# Loading Dataset
file_path = "/content/drive/MyDrive/CVproject/CohortAnalysis/KPMG_VI_New_raw_data_update_final.xlsx"
transaction_df = pd.read_excel(file_path,'Transactions')

# View data
transaction_df.head()

# Set first column as column name
transaction_df.columns = transaction_df.iloc[0]

# drop first column
transaction_df.drop(index=transaction_df.index[0],axis=0,inplace=True)
transaction_df

# Overview of the variables in the dataset
transaction_df.describe()

"""Highlights:

1. Customer ID 2183 placed 14 orders and is the top purchaser in our store.
2. On February 14, 2017, there were 82 orders placed, marking it as the day with the most daily transaction orders.
3. There are a few canceled orders.
4. The brand Solex is the most popular brand in our store
5. The Standard product line has the highest sales, with the medium class being the top-selling class and medium being the most frequently sold product size.
6. The most in-demand product is priced at \$2091 and costs \$388.92 to produce, giving us a profit of \$1702.55.
"""

transaction_df[transaction_df['order_status'] != 'Approved']
# 179 rows out of 20000 rows are cancelled order data

"""### Data cleaning - checking and working with missing value

"""

transaction_df.info()

# Inspect missing values in the dataset
print(transaction_df.isnull().values.sum())

# Check blank space
transaction_df.applymap(lambda x:x=="").sum().sum()

# Replace the " "s with NaN
# transaction_df = transaction_df.replace(" ",np.NaN)

# Count NaNs
transaction_df.isnull().values.sum()

# Treat missing values of numerical variables with mean
# Treat missing values of non-numerical variables with the most frequent value
for col in transaction_df.columns:
  if transaction_df[col].dtypes == 'object':
      transaction_df[col] = transaction_df[col].fillna(transaction_df[col].value_counts().index[0])
# Check the number of NaNs to verify
transaction_df.isnull().values.sum()
# All missing values filled

transaction_df.info()

"""Here, we can see that we have 1542 null values, which we treated with mean as well as most frequent values as per datatype. Also, we correct datatypes for each variables. Now, as we have completed our data cleaning and understanding, we will commence the Exploratory Data Analysis (EDA) and Cohort Analysis.

## Exploratory Data Analysis (EDA)
"""

# Revenue of online and offline orders
online_order_count = transaction_df['online_order'].sum()
total_orders = len(transaction_df)
offline_order_count = total_orders - online_order_count

sizes = [online_order_count, offline_order_count]
labels = ['Online Order', 'Offline Order']
explode = (0.1, 0)  # Only explode the 'Online Order' slice

plt.figure(figsize=(10, 6))
plt.pie(sizes, explode=explode, labels=labels,
autopct='%1.1f%%', shadow=True, startangle=140)

plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Order Types')

# Revenue of each product line
brand_revenue = transaction_df.groupby(['product_line']).sum(numeric_only=True)['list_price'].reset_index()

plt.figure(figsize=(10, 6))
plt.pie(brand_revenue['list_price'], labels=brand_revenue['product_line'], autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Total Revenue by Product Line')
plt.show()

#most profit line
brand_cost = transaction_df.groupby(['product_line']).sum()['standard_cost'].reset_index()
brand_revenue['total_cost'] = brand_cost['standard_cost']
brand_revenue['profit'] = brand_revenue['list_price'] - brand_revenue['total_cost']
brand_revenue

plt.figure(figsize=(10, 6))
plt.pie(brand_revenue['profit'], labels=brand_revenue['product_line'], autopct='%1.1f%%', startangle=140)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.title('Total Profit by Product Line')
plt.show()

# Density of list_price
plt.figure(figsize=(10, 6))
sns.histplot(transaction_df['list_price'], bins=10, kde=True, color='g')
plt.title('Density of List Price')

"""Notes: Products priced from \$1100 to \$1490 generally have the highest sales."""

# revenue of each month
# Convert the 'date' column to datetime if it isn't already
#transaction_df['product_first_sold_date'] = pd.to_datetime(transaction_df['product_first_sold_date'])
# Extract month and add it as a new column
transaction_df['transaction_month'] = transaction_df['transaction_date'].dt.month
# Aggregate data by month
monthly_revenue = transaction_df.groupby(['transaction_month','online_order']).sum(numeric_only=True)['list_price'].reset_index()
monthly_revenue

plt.figure(figsize=(10, 6))
sns.lineplot(x='transaction_month', y='list_price', hue='online_order', data=monthly_revenue, marker="o")
plt.title('Total Revenue by Month')
plt.xlabel('Month')
plt.ylabel('Total Revenue')

"""Notes: For the majority of the year, online sales surpass offline sales."""

plt.figure(figsize=(10, 6))
# Compute total revenue for each brand and sort brands by this total
sorted_brands = transaction_df.groupby('brand')['list_price'].sum().sort_values(ascending=False).index

# Plot using this order
ax = sns.barplot(x='brand', y='list_price', hue='online_order', data=transaction_df, estimator=sum, errorbar=None, order=sorted_brands)
ax.set_title('Total Revenue by Brand and Order Method')

plt.figure(figsize=(10, 6))
sns.relplot(data=transaction_df, x="brand", y="list_price", hue="product_line")

# Rotate x-axis labels
plt.xticks(rotation=45, horizontalalignment='right')
plt.title('Amount of Sale by Brand and Product Line');

"""Notes: Our top-selling brand, Solex, has online order revenue that is marginally less than its offline order revenue."""

# Calculate correlation matrix
corr_matrix = transaction_df.corr(numeric_only=True)

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')

"""Notes:

Highly positive correlated variables: List_price and standard_cost

Weekly positive corrleated variables: list_price and product_id; standard_cost and product_id

## Cohorts Analysis

Cohort analysis allows us to examine the behavior of user groups over time. In this specific analysis, we've segmented our users into monthly cohorts based on when they first engaged with our platform or product. Our primary focus is to understand how well we retain users month-over-month.

###Retention Rate in Percentage: Monthly Cohorts

Assigned the cohorts and calculated the monthly offset
Calculating the time offset for each transaction allows us to evaluate the metrics for each cohort in a comparable fashion.

First, we will create 6 variables that capture the integer value of years, months, and days for Transaction and Cohort Date using the get_date_int() function.
"""

# For performing the Time cohorts, this function will parse the date to get the month
def get_month(x):
  return dt.datetime(x.year, x.month, 1)
# Get month from 'transaction_date' column and store in 'trans_month' column
transaction_df['trans_month'] = transaction_df['transaction_date'].apply(get_month)
transaction_df.info()

# get the first purchase month for each customer and store in 'cohort_month'
transaction_df['cohort_month_date'] = transaction_df.groupby('customer_id')['trans_month'].transform('min')
transaction_df.head()

# Calculate the monthly offsets
# define a function to capture the interger value of years, months, and days for 'transaction_month' and 'cohort_month'
def get_date_int(df, column):
    year = df[column].dt.year
    month = df[column].dt.month
    day = df[column].dt.day
    return year, month, day

transaction_year, transaction_month,_ = get_date_int(transaction_df, 'trans_month')
cohort_year, cohort_month,_ = get_date_int(transaction_df, 'cohort_month_date')

"""Now we will calculate the difference between the Invoice Dates and Cohort dates in years, months separately, then calculate the total Months difference between the two. This will be our monthâ€™s offset or cohort Index, which we will use in the next section to calculate the retention rate."""

# get the difference in year
years_diff = transaction_year - cohort_year
# get the difference in month
months_diff = transaction_month - cohort_month
# get cohort index
""" Extract the difference in months from all previous values
 "+1" in addeded at the end so that first month is marked as 1 instead of 0 for easier interpretation.
 """
transaction_df['CohortIndex'] = years_diff * 12 + months_diff + 1
transaction_df

# Counting monthly active user from each chort
# Counting number of unique customer Id's falling in each group of 'cohort_month_date' and 'CohortIndex'
cohort_data = transaction_df.groupby(['cohort_month_date', 'CohortIndex']).nunique()['customer_id'].reset_index()
cohort_data

# Create pivot table
cohort_counts = cohort_data.pivot(index='cohort_month_date', columns= 'CohortIndex',values = 'customer_id')
cohort_counts

# Calculate retention rate
cohort_sizes = cohort_counts.iloc[:,0]
retention_number = cohort_counts.divide(cohort_sizes, axis=0)
# Coverting the retention rate into percentage and rounding off
retention_rate = retention_number.round(3)*100
retention_rate

retention_rate.index = retention_rate.index.strftime('%Y-%m')
retention_rate

# Create a percentage formatter function
percentage_formatter = lambda x: f'{x:.1f}%'
# Initialize the figure
plt.figure(figsize=(16, 10))
# Adding a title
plt.title('Retention Rate in Percentage: Monthly Cohorts', fontsize = 14)
# Creating the heatmap
sns.heatmap(retention_rate, annot = retention_rate.applymap(percentage_formatter),vmin = 0.0, vmax =100,cmap="YlGnBu", fmt='')
plt.ylabel('Cohort Month')
plt.xlabel('Cohort Index')
plt.yticks(rotation=360)
plt.show()

"""Here, We have 12 cohorts for each month and 12 cohort indexes. The darker the blue shades higher the values.

**Cohorts (Rows)**: "Each row represents a distinct monthly cohort of users.
For instance, the first row represents users who first engaged with us in January."



**Time Periods (Columns)**: "The columns represent the months elapsed since the cohort's first engagement. So, the first column represents the percentage of customer made first pruchase in January so that it's 100%. The second column represents the percentage of January's cohort that returned in February,and so on."

<br>

Key Findings:


1. The overall retention rate falls under 50%, indicating there's room for improvement in reducing our churn rate and enhancing customer loyalty.

2. The retention rate varies significantly across cohorts, with some cohorts having higher or lower retention rates than others. For example, the cohort from Feburary has the highest retention rate, while the cohort from October has the lowest retention rate. This suggests that there are some factors that influence customer behavior and satisfaction depending on when they joined your product or service.

3. The retention rate displays a periodic trend for several months throughout the year. Specifically, cohorts from January to July experience a surge in retention rates during June and July. This pattern implies that certain seasonal factors or marketing efforts during these months may be effectively driving customer engagement and loyalty.


4. The most noticeable decline appears in the 5-month to 6-month period for the July cohort, with a drop of about 17%. This suggests potential challenges or a decrease in the perceived value by customers during this phase. We could deep futher with Root Cause Analysis.

### Root Cause Analysis
"""

# get data of cohort month July, cohort index 5
customerbehavior1 = transaction_df[(transaction_df['cohort_month_date']=='2017-07-01') & (transaction_df['CohortIndex']==5) ]
# get data of cohort month August, cohort index 5
customerbehavior2 = transaction_df[(transaction_df['cohort_month_date']=='2017-08-01') & (transaction_df['CohortIndex']==5) ]

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))

# First plot
sns.histplot(data=customerbehavior1['list_price'], bins=10, kde=True, color='g', ax=axes[0])
axes[0].set_title('Density of List Price: July Cohort with Index 5')

# Secound plot
sns.histplot(customerbehavior2['list_price'], bins=10, kde=True, color='g',ax=axes[1])
axes[1].set_title('Density of List Price: July Cohort with Index 6')

plt.tight_layout() # To ensure that the plots don't overlap

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

# First plot
sns.scatterplot(data=customerbehavior1, x="brand", y="list_price", hue="product_line", ax=axes[0])
axes[0].tick_params(axis='x', rotation=45)
axes[0].set_title('Amount of Sale by Brand and Product Line: July Cohort with Index 5')

# Second plot
sns.scatterplot(data=customerbehavior2, x="brand", y="list_price", hue="product_line", ax=axes[1])
axes[1].tick_params(axis='x', rotation=45)
axes[1].set_title('Amount of Sale by Brand and Product Line: July Cohort with Index 6')

# Adjust layout to ensure that the plots don't overlap and are displayed nicely
plt.tight_layout()

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

# Compute total revenue for each brand and sort brands by this total
sorted_brands1 = customerbehavior1.groupby('brand')['list_price'].sum().sort_values(ascending=False).index

# First plot
sns.barplot(x='brand', y='list_price', hue='online_order', data=customerbehavior1, estimator=sum, errorbar=None, order=sorted_brands1, ax=axes[0])
axes[0].set_title('Total Revenue by Brand and Order Method: July Cohort with Index 6')
axes[0].tick_params(axis='x', rotation=45)
axes[0].legend(loc='upper left', bbox_to_anchor=(1, 1))

# Second plot
sns.barplot(x='brand', y='list_price', hue='online_order', data=customerbehavior2, estimator=sum, errorbar=None, order=sorted_brands1, ax=axes[1])
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, horizontalalignment='right')
axes[1].legend(loc='upper left', bbox_to_anchor=(1, 1))
axes[0].set_title('Total Revenue by Brand and Order Method: July Cohort with Index 5')

plt.tight_layout()

"""From the above charts, we can see that:

There's been a decline in the buying behavior of both high-priced and low-priced items.Sales of standard products have seen a reduction. Solex, the top-selling brand in the shop, has experienced a noticeable drop in online sales, while three other brands lack offline sales entirely.
"""

# Average churn rate for the customers based on the cohort analysis
average_percent_of_customers = retention_rate.mean(axis=0).values[1:]
month_since_first_purchase = retention_rate.columns[1:]

plt.plot(month_since_first_purchase, average_percent_of_customers)
plt.ylabel('Retention Rate')
plt.xlabel('Months Since First Purchase')
plt.title('Average Customer Retention Rate', fontsize = 14);

"""From the above chart, we can see that:

1. The customer retention rate consistently hovers between 35.5% and 39%.

2. Approximately 39% of customers make repeat purchases at the peak at 4 months, and this drops to its lowest at around 35.7% at 6 months.

3. After that, it remains fairly stable between 37% to 39%.

Possible reasons include:

1. Bike shop's seasonal promotional activities;

2. Bicycle parts replacement and upgrades in a regular basis;

3. Changes in customers' riding habits.

## Strategy & Future Analysis

Stragegy


Given the observed 4-monthly decline in retention rates, we suggest promoting subscriptions to our website and providing discounts to bolster customer loyalty. The discount structure could be tailored based on a set standard, targeting both premium and budget-friendly products. As Solex is our top-selling brand, a collaboration for marketing campaigns and exclusive promotions could be beneficial. This partnership might also allow us to secure better pricing advantages.

Futhermore, cultivate a community centered on bicycles using social media platforms and related events. Provide educational sessions, including tutorials and webinars to help customers maximize the benefits of the product or service. Ensure clients are well-informed about all the available products that they might have overlooked.

<br>

For the future Analysis, a direction is to work with other department to gather more data and identify causes:



*   Product Quality: Are there products that are being returned often or have negative reviews?
*   Customer Service: Are customers having a bad shopping experience?
*   Usability: Is the shop's website/app user-friendly?
*   Marketing: Were there any misleading marketing campaigns that could have set wrong expectations?
*   Price: Are products priced competitively?
*   External Factors: Are there external events or market changes influencing the shopping behavior?

<br>

Other analysis we could do:

* A/B Testing: If a specific part of the shopping experience might be the cause, conduct A/B tests to validate. A/B Tesing also can help analyze which marketing campaigns are effective and why.

* Machine Learning: Using K-means clustering, Hierarchical clustering, DBSCAN to cluster customers into distinct groups based on purchasing behavior, demographics, engagement metrics, etc.Using Time series analysis, ARIMA, Prophet to predict future demands and foresee trends.

* Competitive Analysis: Look at competitors' retention rates and strategies.
"""